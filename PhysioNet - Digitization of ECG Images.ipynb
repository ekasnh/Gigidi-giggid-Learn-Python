{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNK50pEgjOqpa3Rl7XM/LFU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370},"id":"f-S9Szqsa_C_","executionInfo":{"status":"error","timestamp":1766531909052,"user_tz":-330,"elapsed":68639,"user":{"displayName":"Rishivar Khurana","userId":"16486220586179176001"}},"outputId":"1d8f5584-7c09-496f-83db-31b6474d7491"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Please upload your kaggle.json file:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-affc5dc2-2186-4f73-87b3-0426a89f9e5b\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-affc5dc2-2186-4f73-87b3-0426a89f9e5b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n","https://www.kaggle.com/competitions/physionet-ecg-image-digitization                2026-01-22 23:59:00  Research            50,000 Usd        823            True  \n","Downloading dataset for physionet-ecg-image-digitization...\n","^C\n","Extracting files...\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"Download failed: physionet-ecg-image-digitization.zip not found after kaggle download command.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-588327337.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Check if the zip file exists after download attempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Download failed: {zip_file_path} not found after kaggle download command.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Download failed: physionet-ecg-image-digitization.zip not found after kaggle download command."]}],"source":["# ==========================================\n","# 1. ENVIRONMENT SETUP\n","# ==========================================\n","# We install the necessary libraries for data processing,\n","# image handling, and deep learning.\n","!pip install -q kaggle numpy pandas opencv-python matplotlib scikit-learn torch torchvision tqdm wfdb albumentations\n","\n","import os\n","import zipfile\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models, transforms\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import train_test_split\n","from google.colab import files\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# ==========================================\n","# 2. KAGGLE AUTHENTICATION\n","# ==========================================\n","# IMPORTANT: You must have a 'kaggle.json' file from your Kaggle account.\n","# Account -> Create New API Token.\n","print(\"Please upload your kaggle.json file:\")\n","uploaded = files.upload()\n","\n","if 'kaggle.json' not in uploaded:\n","    raise FileNotFoundError(\"kaggle.json not found. Please upload the correct file.\")\n","\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","# Verify the CLI works\n","!kaggle competitions list | grep \"physionet-ecg-image-digitization\"\n","\n","# ==========================================\n","# 3. DATASET DOWNLOAD\n","# ==========================================\n","COMPETITION_NAME = \"physionet-ecg-image-digitization\"\n","\n","print(f\"Downloading dataset for {COMPETITION_NAME}...\")\n","\n","# Ensure any partially downloaded or existing zip is removed to allow a fresh download\n","zip_file_path = f\"{COMPETITION_NAME}.zip\"\n","if os.path.exists(zip_file_path):\n","    print(f\"Removing existing {zip_file_path} before re-downloading.\")\n","    !rm {zip_file_path}\n","\n","# Re-attempt download\n","!kaggle competitions download -c {COMPETITION_NAME}\n","\n","# Unzip the data\n","print(\"Extracting files...\")\n","# Check if the zip file exists after download attempt\n","if not os.path.exists(zip_file_path):\n","    raise FileNotFoundError(f\"Download failed: {zip_file_path} not found after kaggle download command.\")\n","\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extractall(\"data\")\n","\n","# Clean up zip\n","!rm {zip_file_path}\n","\n","print(\"Directory structure:\")\n","!ls -R data | head -n 20\n","\n","# ==========================================\n","# 4. DATA EXPLORATION\n","# ==========================================\n","# We check the train.csv and test.csv and visualize some ECG images.\n","# In this competition, the goal is to extract time-series (digitization).\n","train_meta = pd.read_csv('data/train.csv')\n","test_meta = pd.read_csv('data/test.csv')\n","\n","print(f\"Train samples: {len(train_meta)}\")\n","print(f\"Test samples: {len(test_meta)}\")\n","print(train_meta.head())\n","\n","def plot_samples(df, num=3):\n","    plt.figure(figsize=(15, 5))\n","    for i in range(num):\n","        img_id = df.iloc[i]['base_id']\n","        # Note: The actual path might differ slightly based on unzip folder structure\n","        # We search for the image file\n","        img_path = f\"data/train_images/{img_id}.png\"\n","        if not os.path.exists(img_path):\n","            img_path = f\"data/train_images/{img_id}.jpg\"\n","\n","        if os.path.exists(img_path):\n","            img = cv2.imread(img_path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            plt.subplot(1, num, i+1)\n","            plt.imshow(img)\n","            plt.title(f\"ID: {img_id}\")\n","            plt.axis('off')\n","    plt.show()\n","\n","# Visualize images (if the train_images folder exists)\n","# Note: In the actual competition, paths are often 'data/train_images/'\n","if os.path.exists('data/train_images'):\n","    plot_samples(train_meta)\n","\n","# ==========================================\n","# 5. PREPROCESSING PIPELINE\n","# ==========================================\n","# We define a PyTorch Dataset.\n","# Digitization is a regression task where we map image pixels to signal values.\n","# For a baseline, we'll resize images to 256x256 and predict a fixed-size vector.\n","\n","class ECGDigitizationDataset(Dataset):\n","    def __init__(self, df, img_dir, transform=None, is_test=False):\n","        self.df = df\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.is_test = is_test\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        img_id = row['base_id']\n","        img_path = os.path.join(self.img_dir, f\"{img_id}.png\")\n","        if not os.path.exists(img_path):\n","            img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n","\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        if self.is_test:\n","            return image, img_id\n","\n","        # Ground Truth: In a real baseline, you would load the .mat or .csv signals.\n","        # Here we create a dummy target to illustrate the training loop.\n","        # In this competition, signals are 12 leads.\n","        target = torch.zeros(12, 100) # Dummy: 12 leads, 100 points each\n","        return image, target\n","\n","# Transformations\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# ==========================================\n","# 6. BASELINE MODEL\n","# ==========================================\n","# A simple ResNet18 backbone with a regression head.\n","class ECGBaselineModel(nn.Module):\n","    def __init__(self, output_size=(12, 100)):\n","        super(ECGBaselineModel, self).__init__()\n","        self.backbone = models.resnet18(pretrained=True)\n","        num_features = self.backbone.fc.in_features\n","        self.backbone.fc = nn.Linear(num_features, output_size[0] * output_size[1])\n","        self.output_size = output_size\n","\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        x = x.view(-1, self.output_size[0], self.output_size[1])\n","        return x\n","\n","model = ECGBaselineModel().to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","# ==========================================\n","# 7. TRAINING & EVALUATION\n","# ==========================================\n","# Using a subset for demonstration.\n","train_df, val_df = train_test_split(train_meta, test_size=0.1, random_state=42)\n","\n","# Set actual image directories found in your 'data' folder\n","# (Adjust these based on the actual zip extraction names)\n","TRAIN_IMG_DIR = \"data/train_images\"\n","VAL_IMG_DIR = \"data/train_images\"\n","TEST_IMG_DIR = \"data/test_images\"\n","\n","# Check if directories exist, if not, try common subfolders\n","if not os.path.exists(TRAIN_IMG_DIR):\n","    TRAIN_IMG_DIR = \"data/images/train\"\n","    TEST_IMG_DIR = \"data/images/test\"\n","\n","train_ds = ECGDigitizationDataset(train_df, TRAIN_IMG_DIR, transform=transform)\n","train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n","\n","print(\"Starting training baseline...\")\n","model.train()\n","for epoch in range(1): # Reduced epochs for baseline execution\n","    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n","    for imgs, targets in pbar:\n","        imgs, targets = imgs.to(device), targets.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(imgs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        pbar.set_postfix(loss=loss.item())\n","\n","torch.save(model.state_dict(), \"ecg_baseline.pth\")\n","\n","# ==========================================\n","# 8. SUBMISSION PREPARATION\n","# ==========================================\n","# The submission expects: id (base_id_rowid_lead) and value.\n","# Lead II is 10s, others 2.5s.\n","leads = [\"I\", \"II\", \"III\", \"aVR\", \"aVL\", \"aVF\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\"]\n","\n","print(\"Generating predictions on test data...\")\n","test_ds = ECGDigitizationDataset(test_meta, TEST_IMG_DIR, transform=transform, is_test=True)\n","test_loader = DataLoader(test_ds, batch_size=8, shuffle=False)\n","\n","model.eval()\n","submission_data = []\n","\n","with torch.no_grad():\n","    for imgs, img_ids in tqdm(test_loader):\n","        imgs = imgs.to(device)\n","        preds = model(imgs).cpu().numpy() # Shape: (B, 12, 100)\n","\n","        for i in range(len(img_ids)):\n","            base_id = img_ids[i]\n","            for l_idx, lead_name in enumerate(leads):\n","                # For baseline, we take 100 predicted points\n","                # and map them to the required indices\n","                signal_points = preds[i, l_idx, :]\n","\n","                # In competition, Lead II has 10*fs points, others 2.5*fs\n","                # We simply map our 100 points for the sample CSV\n","                for row_id, val in enumerate(signal_points):\n","                    submission_data.append({\n","                        \"id\": f\"{base_id}_{row_id}_{lead_name}\",\n","                        \"target\": val\n","                    })\n","\n","submission_df = pd.DataFrame(submission_data)\n","submission_df.to_csv(\"submission.csv\", index=False)\n","\n","print(\"\\nSubmission file preview:\")\n","print(submission_df.head())\n","print(f\"\\nFinal submission saved to submission.csv. Total rows: {len(submission_df)}\")\n","\n","# ==========================================\n","# 9. FINAL NOTES\n","# ==========================================\n","# 1. This is a structural baseline.\n","# 2. To improve: Load real ground truth signals using 'wfdb' or from provided CSVs.\n","# 3. Use actual 'fs' (sampling frequency) to determine the number of points per lead.\n","# 4. Use more complex backbones (ResNet50, EfficientNet) and longer sequences."]}]}